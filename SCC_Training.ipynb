{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947c7706-af35-4f8e-a385-e315cd5946b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB, FrequencyMasking, TimeMasking\n",
    "from torch.optim import AdamW\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d2e325-85e9-4b42-9627-50650ade6809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, audio_folder, transforms=None, target_length=16000):\n",
    "        self.csv_file = csv_file\n",
    "        self.audio_folder = audio_folder\n",
    "        self.target_length = target_length\n",
    "        self.transforms = transforms\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.LABEL_MAPPING = {label: idx for idx, label in enumerate(sorted(self.data['label'].unique()))}\n",
    "#        print(self.data['label'].unique())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.audio_folder, self.data.iloc[idx, 0])\n",
    "        label = self.data.iloc[idx, 1]\n",
    "        label = self.LABEL_MAPPING[label]\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "        if self.transforms:\n",
    "            waveform = self.transforms(waveform)\n",
    "            \n",
    "        return waveform, label\n",
    "\n",
    "    def get_class_weights(self, subset_indices):\n",
    "        \"\"\"\n",
    "        Calculate class weights based on the subset indices (e.g., training set).\n",
    "        \"\"\"\n",
    "        labels = self.data.iloc[subset_indices]['label']\n",
    "        # Calculate class counts for the subset (train set)\n",
    "        class_counts = Counter(labels)\n",
    "        total_samples = len(labels)\n",
    "        num_classes = len(self.LABEL_MAPPING)\n",
    "        \n",
    "        # Calculate inverse frequency (class weights)\n",
    "        class_weights = {label: total_samples / (num_classes * count) for label, count in class_counts.items()}\n",
    "        \n",
    "        # Sort class weights by label index to match the LABEL_MAPPING order\n",
    "        class_weights_tensor = torch.tensor(\n",
    "            [class_weights.get(label, 1.0) for label in sorted(self.LABEL_MAPPING.keys())],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        return class_weights_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308c4418-4072-4cd4-80e8-ba2fb8c6aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech Command CNN Architecture\n",
    "class SpeechCommandCNN(nn.Module):\n",
    "    def __init__(self, n_channels=1, n_classes=10, n_filters=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_channels, n_filters, kernel_size=(3, 3), padding=(1, 1), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(n_filters)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(n_filters, n_filters, kernel_size=(3, 3), padding=(1, 1), bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(n_filters)\n",
    "        self.pool1 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(n_filters, 2 * n_filters, kernel_size=(3, 3), padding=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(2 * n_filters)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(2 * n_filters, 2 * n_filters, kernel_size=(3, 3), padding=(1, 1), bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(2 * n_filters)\n",
    "        self.pool2 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.fc1 = nn.Linear(2 * n_filters, 128)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "       # print(f\"Input shape: {x.shape}\")\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "       # print(f\"After conv1: {x.shape}\")\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool1(x)\n",
    "       # print(f\"After conv2: {x.shape}\")\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "       # print(f\"After conv3: {x.shape}\")\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool2(x)\n",
    "       # print(f\"After conv4: {x.shape}\")\n",
    "\n",
    "        # Global average pooling to reduce Height and Width to 1X1\n",
    "        x = F.avg_pool2d(x, (x.shape[-2], x.shape[-1]))  # Global average pooling\n",
    "       # print(f\"After global avg pool: {x.shape}\")\n",
    "\n",
    "        # Flatten the tensor to pass to the FC layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten to [batch_size, feature maps]\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # Final FC layer (no activation here, as this outputs logits)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5499ff47-ef9d-48a1-b445-7294773fe3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for waveforms, labels in dataloader:\n",
    "        waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(waveforms)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss and compute accuracy\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    avg_train_loss = train_loss / len(dataloader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    return avg_train_loss, train_accuracy\n",
    "\n",
    "\n",
    "# Validation Function\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for waveforms, labels in dataloader:\n",
    "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "            outputs = model(waveforms)         \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / len(dataloader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return avg_val_loss, val_accuracy\n",
    "\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=40, min_delta=0.001, save_path=\"best_model.pth\"):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float(\"inf\")  # Initialize with infinity\n",
    "        self.early_stop = False\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.save_path)  # Save best model weights\n",
    "            print(f\"New best model saved with loss {val_loss:.4f}\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44286393-26fb-4fde-ac2b-f235ca945f78",
   "metadata": {},
   "source": [
    "You can train the model using either regular Cross-Entropy Loss or Weighted Cross-Entropy Loss (faster convergence, higher val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93852d7a-617d-41cd-8838-46d97fce963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Cross-Entropy Training\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"Path_to_your\\\\labels.csv\"\n",
    "    audio_folder = \"Path_to_your\\\\dataset\"\n",
    "\n",
    "# Training transforms with augmentation\n",
    "    train_transforms = torch.nn.Sequential(\n",
    "        MelSpectrogram(sample_rate=16000, n_mels=64, n_fft=2048, hop_length=400),\n",
    "        AmplitudeToDB(),\n",
    "        FrequencyMasking(freq_mask_param=30),\n",
    "        TimeMasking(time_mask_param=30)\n",
    "    )\n",
    "\n",
    "# Validation transforms without augmentation\n",
    "    val_transforms = torch.nn.Sequential(\n",
    "        MelSpectrogram(sample_rate=16000, n_mels=64, n_fft=2048, hop_length=400),\n",
    "        AmplitudeToDB()\n",
    "    )\n",
    "\n",
    "# Create training and validation datasets with their respective transforms\n",
    "    train_dataset = AudioDataset(csv_file=csv_file, audio_folder=audio_folder, transforms=train_transforms)\n",
    "    val_dataset = AudioDataset(csv_file=csv_file, audio_folder=audio_folder, transforms=val_transforms)\n",
    "\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    num_classes = len(pd.unique(pd.read_csv(csv_file)['label']))\n",
    "    model = SpeechCommandCNN(n_classes=num_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=30, min_delta=0.001)\n",
    "\n",
    "    for epoch in range(200):\n",
    "        # Train and get training loss and accuracy\n",
    "        train_loss, train_accuracy = train(model, train_dataloader, criterion, optimizer, device)\n",
    "\n",
    "        # Validate and get validation loss and accuracy\n",
    "        val_loss, val_accuracy = validate(model, val_dataloader, criterion, device)\n",
    "\n",
    "        # Print both losses and accuracies\n",
    "        print(f\"Epoch {epoch + 1} - Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}%\")\n",
    "        print(f\"Epoch {epoch + 1} - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "        # Check early stopping\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2385dc-1067-4d32-8252-5caa9010ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Cross-Entropy Training \n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"labels.csv\"\n",
    "    audio_folder = \"dataset\"\n",
    "\n",
    "    # Training transforms with augmentation\n",
    "    train_transforms = torch.nn.Sequential(\n",
    "        MelSpectrogram(sample_rate=16000, n_mels=64, n_fft=2048, hop_length=400),\n",
    "        AmplitudeToDB(),\n",
    "        FrequencyMasking(freq_mask_param=30),\n",
    "        TimeMasking(time_mask_param=30)\n",
    "    )\n",
    "\n",
    "    # Validation transforms without augmentation\n",
    "    val_transforms = torch.nn.Sequential(\n",
    "        MelSpectrogram(sample_rate=16000, n_mels=64, n_fft=2048, hop_length=400),\n",
    "        AmplitudeToDB()\n",
    "    )\n",
    "\n",
    "    # Create original dataset (without transforms for class weight calculation)\n",
    "    full_dataset = AudioDataset(csv_file=csv_file, audio_folder=audio_folder, transforms=None)\n",
    "\n",
    "    # Split dataset into training (80%) and validation (20%)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    # Get the indices for the training set\n",
    "    train_indices = train_dataset.indices\n",
    "\n",
    "    # Get class weights based on the training subset\n",
    "    class_weights = full_dataset.get_class_weights(subset_indices=train_indices)\n",
    "\n",
    "    # Apply transformations for training and validation datasets\n",
    "    train_dataset = AudioDataset(csv_file=csv_file, audio_folder=audio_folder, transforms=train_transforms)\n",
    "    val_dataset = AudioDataset(csv_file=csv_file, audio_folder=audio_folder, transforms=val_transforms)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    num_classes = len(pd.unique(pd.read_csv(csv_file)['label']))\n",
    "    model = SpeechCommandCNN(n_classes=num_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # CrossEntropyLoss with class weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "    val_criterion = nn.CrossEntropyLoss() # For validation use normal crossentropy to avoid biasing the inference results \n",
    "    optimizer = AdamW(model.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=30, min_delta=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):\n",
    "        # Train and get training loss and accuracy\n",
    "        train_loss, train_accuracy = train(model, train_dataloader, criterion, optimizer, device)\n",
    "\n",
    "        # Validate and get validation loss and accuracy\n",
    "        val_loss, val_accuracy = validate(model, val_dataloader, val_criterion, device)\n",
    "\n",
    "        # Print both losses and accuracies\n",
    "        print(f\"Epoch {epoch + 1} - Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}%\")\n",
    "        print(f\"Epoch {epoch + 1} - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}%\")\n",
    "\n",
    "        # Check early stopping\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc387838-370a-4ee2-9ba0-f59c9153577c",
   "metadata": {},
   "source": [
    "The cell below allows you to convert a pytorch model into an onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8639ba4-f385-47f4-a14a-c071b4c4be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "best_model = SpeechCommandCNN(n_classes=8)\n",
    "best_model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "example_inputs = (torch.randn(1, 1, 64, 41).to(device),) # it has to be representative of your input tensor\n",
    "# Export to ONNX\n",
    "onnx_filename = \"model.onnx\"\n",
    "torch.onnx.export(best_model, example_inputs, onnx_filename, dynamo=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
